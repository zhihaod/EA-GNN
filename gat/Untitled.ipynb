{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import load_data, accuracy\n",
    "from models import GAT, SpGAT\n",
    "import math\n",
    "from torch_geometric.datasets import *\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test,edges = load_data(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zhihao/Document/gnn_fd/graphSage/split/multihead/NC-GNN/gat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "    dataset = Planetoid(root='./data', name='cora')\n",
    "   # dataset = Coauthor('./data/academic_cs', name='CS')\n",
    "    feat_data = dataset[0].x.numpy()\n",
    "    labels = np.expand_dims(dataset[0].y.numpy(), axis=1)\n",
    "    edge = dataset[0].edge_index.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1433)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10556)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sp.coo_matrix((np.ones(edge.shape[1]), (edge[0,:], edge[1,:])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10556,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.nonzero()[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Tensor' and 'dia_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-173e8665001d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Tensor' and 'dia_matrix'"
     ]
    }
   ],
   "source": [
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adj_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-de9b6633f9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madj_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adj_' is not defined"
     ]
    }
   ],
   "source": [
    "adj_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 10556 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'coo_matrix' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ec1aaefc53eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madj_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "adj_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-aa8d2922b454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for seed in range(10):\n",
    "    adj, features, labels, idx_train, idx_val, idx_test,_ = load_data(seed=seed)\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=128, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=0, \n",
    "                nheads=8, \n",
    "                alpha=0.2)\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.005, \n",
    "                       weight_decay=5e-4)\n",
    "    features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()   \n",
    "    \n",
    "    \n",
    "    \n",
    "    t_total = time.time()\n",
    "    loss_values = []\n",
    "    bad_counter = 0\n",
    "    best = 200 + 1\n",
    "    best_epoch = 0\n",
    "    for epoch in range(200):\n",
    "        loss_values.append(train(epoch))\n",
    "    \n",
    "        torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "        if loss_values[-1] < best:\n",
    "            best = loss_values[-1]\n",
    "            best_epoch = epoch\n",
    "            bad_counter = 0\n",
    "        else:\n",
    "            bad_counter += 1\n",
    "    \n",
    "        if bad_counter == 100:\n",
    "            break\n",
    "    \n",
    "        files = glob.glob('*.pkl')\n",
    "        for file in files:\n",
    "            epoch_nb = int(file.split('.')[0])\n",
    "            if epoch_nb < best_epoch:\n",
    "                os.remove(file)\n",
    "    \n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb > best_epoch:\n",
    "            os.remove(file)\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    \n",
    "    # Restore best model\n",
    "    print('Loading {}th epoch'.format(best_epoch))\n",
    "    model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "    \n",
    "    # Testing\n",
    "    acc_list.append(compute_test())    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 5, 4,  ..., 1, 0, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.83593749"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.90234375, 0.890625  , 0.85742188, 0.88476562, 0.8828125 ,\n",
    "       0.89257812, 0.8828125 , 0.88476562, 0.8828125 , 0.875])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8872458410351202,\n",
       " 0.88909426987061,\n",
       " 0.8521256931608133,\n",
       " 0.8964879852125693,\n",
       " 0.8743068391866913,\n",
       " 0.878003696857671,\n",
       " 0.8706099815157117,\n",
       " 0.8817005545286506,\n",
       " 0.8632162661737522,\n",
       " 0.8706099815157117]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GAT(nfeat=features.shape[1], \n",
    "                nhid=128, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=0.6, \n",
    "                nheads=8, \n",
    "                alpha=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.001, \n",
    "                       weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=128, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=0, \n",
    "                nheads=8, \n",
    "                alpha=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.005, \n",
    "                       weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "#    print('Epoch: {:04d}'.format(epoch+1),\n",
    "#          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "#          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "#          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "#          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "#          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    return acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9457 acc_train: 0.1471 loss_val: 1.9436 acc_val: 0.1771 time: 0.0508s\n",
      "Epoch: 0002 loss_train: 1.8819 acc_train: 0.4228 loss_val: 1.8809 acc_val: 0.4336 time: 0.0515s\n",
      "Epoch: 0003 loss_train: 1.8215 acc_train: 0.4129 loss_val: 1.8215 acc_val: 0.4022 time: 0.0447s\n",
      "Epoch: 0004 loss_train: 1.7615 acc_train: 0.4135 loss_val: 1.7627 acc_val: 0.4077 time: 0.0514s\n",
      "Epoch: 0005 loss_train: 1.7010 acc_train: 0.4197 loss_val: 1.7033 acc_val: 0.4151 time: 0.0516s\n",
      "Epoch: 0006 loss_train: 1.6396 acc_train: 0.4326 loss_val: 1.6436 acc_val: 0.4317 time: 0.0521s\n",
      "Epoch: 0007 loss_train: 1.5776 acc_train: 0.4585 loss_val: 1.5838 acc_val: 0.4557 time: 0.0515s\n",
      "Epoch: 0008 loss_train: 1.5148 acc_train: 0.4966 loss_val: 1.5238 acc_val: 0.4963 time: 0.0529s\n",
      "Epoch: 0009 loss_train: 1.4509 acc_train: 0.5305 loss_val: 1.4637 acc_val: 0.5258 time: 0.0481s\n",
      "Epoch: 0010 loss_train: 1.3856 acc_train: 0.5674 loss_val: 1.4030 acc_val: 0.5590 time: 0.0494s\n",
      "Epoch: 0011 loss_train: 1.3187 acc_train: 0.6160 loss_val: 1.3416 acc_val: 0.6052 time: 0.0489s\n",
      "Epoch: 0012 loss_train: 1.2507 acc_train: 0.6572 loss_val: 1.2799 acc_val: 0.6476 time: 0.0507s\n",
      "Epoch: 0013 loss_train: 1.1827 acc_train: 0.7274 loss_val: 1.2184 acc_val: 0.7048 time: 0.0500s\n",
      "Epoch: 0014 loss_train: 1.1161 acc_train: 0.7754 loss_val: 1.1583 acc_val: 0.7399 time: 0.0481s\n",
      "Epoch: 0015 loss_train: 1.0521 acc_train: 0.8148 loss_val: 1.1005 acc_val: 0.7601 time: 0.0479s\n",
      "Epoch: 0016 loss_train: 0.9917 acc_train: 0.8277 loss_val: 1.0458 acc_val: 0.7860 time: 0.0480s\n",
      "Epoch: 0017 loss_train: 0.9350 acc_train: 0.8406 loss_val: 0.9944 acc_val: 0.8026 time: 0.0490s\n",
      "Epoch: 0018 loss_train: 0.8821 acc_train: 0.8480 loss_val: 0.9464 acc_val: 0.8137 time: 0.0497s\n",
      "Epoch: 0019 loss_train: 0.8333 acc_train: 0.8542 loss_val: 0.9017 acc_val: 0.8192 time: 0.0494s\n",
      "Epoch: 0020 loss_train: 0.7884 acc_train: 0.8572 loss_val: 0.8607 acc_val: 0.8229 time: 0.0492s\n",
      "Epoch: 0021 loss_train: 0.7475 acc_train: 0.8609 loss_val: 0.8234 acc_val: 0.8247 time: 0.0485s\n",
      "Epoch: 0022 loss_train: 0.7107 acc_train: 0.8634 loss_val: 0.7899 acc_val: 0.8266 time: 0.0517s\n",
      "Epoch: 0023 loss_train: 0.6776 acc_train: 0.8671 loss_val: 0.7601 acc_val: 0.8284 time: 0.0507s\n",
      "Epoch: 0024 loss_train: 0.6480 acc_train: 0.8708 loss_val: 0.7337 acc_val: 0.8303 time: 0.0471s\n",
      "Epoch: 0025 loss_train: 0.6214 acc_train: 0.8738 loss_val: 0.7102 acc_val: 0.8339 time: 0.0464s\n",
      "Epoch: 0026 loss_train: 0.5977 acc_train: 0.8782 loss_val: 0.6893 acc_val: 0.8358 time: 0.0413s\n",
      "Epoch: 0027 loss_train: 0.5766 acc_train: 0.8818 loss_val: 0.6707 acc_val: 0.8395 time: 0.0451s\n",
      "Epoch: 0028 loss_train: 0.5578 acc_train: 0.8831 loss_val: 0.6541 acc_val: 0.8395 time: 0.0491s\n",
      "Epoch: 0029 loss_train: 0.5411 acc_train: 0.8862 loss_val: 0.6389 acc_val: 0.8413 time: 0.0479s\n",
      "Epoch: 0030 loss_train: 0.5262 acc_train: 0.8898 loss_val: 0.6252 acc_val: 0.8432 time: 0.0487s\n",
      "Epoch: 0031 loss_train: 0.5128 acc_train: 0.8911 loss_val: 0.6128 acc_val: 0.8469 time: 0.0511s\n",
      "Epoch: 0032 loss_train: 0.5007 acc_train: 0.8942 loss_val: 0.6016 acc_val: 0.8506 time: 0.0434s\n",
      "Epoch: 0033 loss_train: 0.4900 acc_train: 0.8972 loss_val: 0.5915 acc_val: 0.8506 time: 0.0448s\n",
      "Epoch: 0034 loss_train: 0.4803 acc_train: 0.8985 loss_val: 0.5824 acc_val: 0.8561 time: 0.0483s\n",
      "Epoch: 0035 loss_train: 0.4716 acc_train: 0.9022 loss_val: 0.5744 acc_val: 0.8579 time: 0.0443s\n",
      "Epoch: 0036 loss_train: 0.4636 acc_train: 0.9040 loss_val: 0.5672 acc_val: 0.8653 time: 0.0481s\n",
      "Epoch: 0037 loss_train: 0.4563 acc_train: 0.9065 loss_val: 0.5606 acc_val: 0.8690 time: 0.0468s\n",
      "Epoch: 0038 loss_train: 0.4496 acc_train: 0.9071 loss_val: 0.5545 acc_val: 0.8708 time: 0.0441s\n",
      "Epoch: 0039 loss_train: 0.4433 acc_train: 0.9077 loss_val: 0.5487 acc_val: 0.8727 time: 0.0475s\n",
      "Epoch: 0040 loss_train: 0.4373 acc_train: 0.9083 loss_val: 0.5433 acc_val: 0.8727 time: 0.0479s\n",
      "Epoch: 0041 loss_train: 0.4316 acc_train: 0.9102 loss_val: 0.5381 acc_val: 0.8708 time: 0.0472s\n",
      "Epoch: 0042 loss_train: 0.4262 acc_train: 0.9108 loss_val: 0.5333 acc_val: 0.8708 time: 0.0491s\n",
      "Epoch: 0043 loss_train: 0.4211 acc_train: 0.9114 loss_val: 0.5288 acc_val: 0.8727 time: 0.0466s\n",
      "Epoch: 0044 loss_train: 0.4162 acc_train: 0.9126 loss_val: 0.5248 acc_val: 0.8727 time: 0.0467s\n",
      "Epoch: 0045 loss_train: 0.4115 acc_train: 0.9138 loss_val: 0.5210 acc_val: 0.8745 time: 0.0432s\n",
      "Epoch: 0046 loss_train: 0.4072 acc_train: 0.9138 loss_val: 0.5175 acc_val: 0.8745 time: 0.0458s\n",
      "Epoch: 0047 loss_train: 0.4030 acc_train: 0.9132 loss_val: 0.5142 acc_val: 0.8745 time: 0.0481s\n",
      "Epoch: 0048 loss_train: 0.3990 acc_train: 0.9145 loss_val: 0.5111 acc_val: 0.8745 time: 0.0478s\n",
      "Epoch: 0049 loss_train: 0.3952 acc_train: 0.9145 loss_val: 0.5080 acc_val: 0.8745 time: 0.0484s\n",
      "Epoch: 0050 loss_train: 0.3915 acc_train: 0.9163 loss_val: 0.5052 acc_val: 0.8745 time: 0.0498s\n",
      "Epoch: 0051 loss_train: 0.3881 acc_train: 0.9169 loss_val: 0.5027 acc_val: 0.8745 time: 0.0453s\n",
      "Epoch: 0052 loss_train: 0.3847 acc_train: 0.9182 loss_val: 0.5004 acc_val: 0.8745 time: 0.0470s\n",
      "Epoch: 0053 loss_train: 0.3816 acc_train: 0.9188 loss_val: 0.4982 acc_val: 0.8727 time: 0.0440s\n",
      "Epoch: 0054 loss_train: 0.3785 acc_train: 0.9206 loss_val: 0.4961 acc_val: 0.8727 time: 0.0457s\n",
      "Epoch: 0055 loss_train: 0.3756 acc_train: 0.9206 loss_val: 0.4940 acc_val: 0.8708 time: 0.0477s\n",
      "Epoch: 0056 loss_train: 0.3727 acc_train: 0.9212 loss_val: 0.4920 acc_val: 0.8727 time: 0.0476s\n",
      "Epoch: 0057 loss_train: 0.3700 acc_train: 0.9212 loss_val: 0.4900 acc_val: 0.8708 time: 0.0441s\n",
      "Epoch: 0058 loss_train: 0.3673 acc_train: 0.9218 loss_val: 0.4882 acc_val: 0.8708 time: 0.0475s\n",
      "Epoch: 0059 loss_train: 0.3648 acc_train: 0.9218 loss_val: 0.4866 acc_val: 0.8708 time: 0.0474s\n",
      "Epoch: 0060 loss_train: 0.3623 acc_train: 0.9206 loss_val: 0.4851 acc_val: 0.8708 time: 0.0494s\n",
      "Epoch: 0061 loss_train: 0.3600 acc_train: 0.9212 loss_val: 0.4835 acc_val: 0.8745 time: 0.0476s\n",
      "Epoch: 0062 loss_train: 0.3578 acc_train: 0.9212 loss_val: 0.4820 acc_val: 0.8745 time: 0.0442s\n",
      "Epoch: 0063 loss_train: 0.3556 acc_train: 0.9225 loss_val: 0.4805 acc_val: 0.8745 time: 0.0468s\n",
      "Epoch: 0064 loss_train: 0.3535 acc_train: 0.9218 loss_val: 0.4790 acc_val: 0.8745 time: 0.0483s\n",
      "Epoch: 0065 loss_train: 0.3515 acc_train: 0.9225 loss_val: 0.4777 acc_val: 0.8745 time: 0.0474s\n",
      "Epoch: 0066 loss_train: 0.3495 acc_train: 0.9237 loss_val: 0.4765 acc_val: 0.8745 time: 0.0432s\n",
      "Epoch: 0067 loss_train: 0.3477 acc_train: 0.9237 loss_val: 0.4753 acc_val: 0.8745 time: 0.0476s\n",
      "Epoch: 0068 loss_train: 0.3459 acc_train: 0.9243 loss_val: 0.4742 acc_val: 0.8745 time: 0.0482s\n",
      "Epoch: 0069 loss_train: 0.3441 acc_train: 0.9237 loss_val: 0.4731 acc_val: 0.8745 time: 0.0469s\n",
      "Epoch: 0070 loss_train: 0.3424 acc_train: 0.9243 loss_val: 0.4720 acc_val: 0.8764 time: 0.0429s\n",
      "Epoch: 0071 loss_train: 0.3407 acc_train: 0.9243 loss_val: 0.4709 acc_val: 0.8764 time: 0.0457s\n",
      "Epoch: 0072 loss_train: 0.3391 acc_train: 0.9237 loss_val: 0.4700 acc_val: 0.8764 time: 0.0495s\n",
      "Epoch: 0073 loss_train: 0.3375 acc_train: 0.9243 loss_val: 0.4691 acc_val: 0.8764 time: 0.0479s\n",
      "Epoch: 0074 loss_train: 0.3359 acc_train: 0.9243 loss_val: 0.4683 acc_val: 0.8764 time: 0.0466s\n",
      "Epoch: 0075 loss_train: 0.3345 acc_train: 0.9268 loss_val: 0.4675 acc_val: 0.8764 time: 0.0474s\n",
      "Epoch: 0076 loss_train: 0.3331 acc_train: 0.9262 loss_val: 0.4667 acc_val: 0.8782 time: 0.0489s\n",
      "Epoch: 0077 loss_train: 0.3318 acc_train: 0.9243 loss_val: 0.4659 acc_val: 0.8782 time: 0.0485s\n",
      "Epoch: 0078 loss_train: 0.3306 acc_train: 0.9243 loss_val: 0.4651 acc_val: 0.8764 time: 0.0491s\n",
      "Epoch: 0079 loss_train: 0.3294 acc_train: 0.9237 loss_val: 0.4645 acc_val: 0.8764 time: 0.0481s\n",
      "Epoch: 0080 loss_train: 0.3282 acc_train: 0.9237 loss_val: 0.4638 acc_val: 0.8764 time: 0.0472s\n",
      "Epoch: 0081 loss_train: 0.3272 acc_train: 0.9237 loss_val: 0.4631 acc_val: 0.8764 time: 0.0500s\n",
      "Epoch: 0082 loss_train: 0.3262 acc_train: 0.9237 loss_val: 0.4625 acc_val: 0.8764 time: 0.0479s\n",
      "Epoch: 0083 loss_train: 0.3252 acc_train: 0.9243 loss_val: 0.4620 acc_val: 0.8764 time: 0.0470s\n",
      "Epoch: 0084 loss_train: 0.3242 acc_train: 0.9249 loss_val: 0.4615 acc_val: 0.8745 time: 0.0434s\n",
      "Epoch: 0085 loss_train: 0.3233 acc_train: 0.9249 loss_val: 0.4611 acc_val: 0.8782 time: 0.0439s\n",
      "Epoch: 0086 loss_train: 0.3224 acc_train: 0.9249 loss_val: 0.4606 acc_val: 0.8782 time: 0.0484s\n",
      "Epoch: 0087 loss_train: 0.3215 acc_train: 0.9249 loss_val: 0.4601 acc_val: 0.8782 time: 0.0481s\n",
      "Epoch: 0088 loss_train: 0.3207 acc_train: 0.9249 loss_val: 0.4597 acc_val: 0.8782 time: 0.0471s\n",
      "Epoch: 0089 loss_train: 0.3198 acc_train: 0.9255 loss_val: 0.4593 acc_val: 0.8764 time: 0.0433s\n",
      "Epoch: 0090 loss_train: 0.3190 acc_train: 0.9255 loss_val: 0.4590 acc_val: 0.8782 time: 0.0478s\n",
      "Epoch: 0091 loss_train: 0.3182 acc_train: 0.9255 loss_val: 0.4586 acc_val: 0.8782 time: 0.0468s\n",
      "Epoch: 0092 loss_train: 0.3175 acc_train: 0.9249 loss_val: 0.4582 acc_val: 0.8782 time: 0.0496s\n",
      "Epoch: 0093 loss_train: 0.3167 acc_train: 0.9255 loss_val: 0.4578 acc_val: 0.8782 time: 0.0490s\n",
      "Epoch: 0094 loss_train: 0.3160 acc_train: 0.9255 loss_val: 0.4575 acc_val: 0.8782 time: 0.0472s\n",
      "Epoch: 0095 loss_train: 0.3153 acc_train: 0.9255 loss_val: 0.4573 acc_val: 0.8801 time: 0.0432s\n",
      "Epoch: 0096 loss_train: 0.3146 acc_train: 0.9255 loss_val: 0.4571 acc_val: 0.8801 time: 0.0481s\n",
      "Epoch: 0097 loss_train: 0.3140 acc_train: 0.9255 loss_val: 0.4568 acc_val: 0.8801 time: 0.0477s\n",
      "Epoch: 0098 loss_train: 0.3134 acc_train: 0.9255 loss_val: 0.4564 acc_val: 0.8801 time: 0.0482s\n",
      "Epoch: 0099 loss_train: 0.3128 acc_train: 0.9262 loss_val: 0.4562 acc_val: 0.8801 time: 0.0449s\n",
      "Epoch: 0100 loss_train: 0.3122 acc_train: 0.9255 loss_val: 0.4560 acc_val: 0.8801 time: 0.0492s\n",
      "Epoch: 0101 loss_train: 0.3116 acc_train: 0.9268 loss_val: 0.4556 acc_val: 0.8801 time: 0.0484s\n",
      "Epoch: 0102 loss_train: 0.3110 acc_train: 0.9268 loss_val: 0.4554 acc_val: 0.8819 time: 0.0463s\n",
      "Epoch: 0103 loss_train: 0.3104 acc_train: 0.9262 loss_val: 0.4552 acc_val: 0.8819 time: 0.0476s\n",
      "Epoch: 0104 loss_train: 0.3099 acc_train: 0.9262 loss_val: 0.4550 acc_val: 0.8819 time: 0.0489s\n",
      "Epoch: 0105 loss_train: 0.3094 acc_train: 0.9262 loss_val: 0.4548 acc_val: 0.8819 time: 0.0462s\n",
      "Epoch: 0106 loss_train: 0.3089 acc_train: 0.9262 loss_val: 0.4546 acc_val: 0.8819 time: 0.0488s\n",
      "Epoch: 0107 loss_train: 0.3084 acc_train: 0.9274 loss_val: 0.4544 acc_val: 0.8819 time: 0.0484s\n",
      "Epoch: 0108 loss_train: 0.3079 acc_train: 0.9274 loss_val: 0.4541 acc_val: 0.8819 time: 0.0494s\n",
      "Epoch: 0109 loss_train: 0.3075 acc_train: 0.9268 loss_val: 0.4538 acc_val: 0.8819 time: 0.0500s\n",
      "Epoch: 0110 loss_train: 0.3071 acc_train: 0.9274 loss_val: 0.4537 acc_val: 0.8819 time: 0.0486s\n",
      "Epoch: 0111 loss_train: 0.3066 acc_train: 0.9274 loss_val: 0.4538 acc_val: 0.8819 time: 0.0485s\n",
      "Epoch: 0112 loss_train: 0.3062 acc_train: 0.9280 loss_val: 0.4537 acc_val: 0.8819 time: 0.0489s\n",
      "Epoch: 0113 loss_train: 0.3058 acc_train: 0.9286 loss_val: 0.4536 acc_val: 0.8819 time: 0.0429s\n",
      "Epoch: 0114 loss_train: 0.3054 acc_train: 0.9286 loss_val: 0.4535 acc_val: 0.8819 time: 0.0473s\n",
      "Epoch: 0115 loss_train: 0.3050 acc_train: 0.9286 loss_val: 0.4535 acc_val: 0.8819 time: 0.0472s\n",
      "Epoch: 0116 loss_train: 0.3047 acc_train: 0.9274 loss_val: 0.4536 acc_val: 0.8819 time: 0.0475s\n",
      "Epoch: 0117 loss_train: 0.3043 acc_train: 0.9286 loss_val: 0.4537 acc_val: 0.8819 time: 0.0446s\n",
      "Epoch: 0118 loss_train: 0.3040 acc_train: 0.9286 loss_val: 0.4538 acc_val: 0.8838 time: 0.0474s\n",
      "Epoch: 0119 loss_train: 0.3036 acc_train: 0.9280 loss_val: 0.4537 acc_val: 0.8838 time: 0.0476s\n",
      "Epoch: 0120 loss_train: 0.3032 acc_train: 0.9286 loss_val: 0.4537 acc_val: 0.8838 time: 0.0477s\n",
      "Epoch: 0121 loss_train: 0.3028 acc_train: 0.9292 loss_val: 0.4537 acc_val: 0.8838 time: 0.0477s\n",
      "Epoch: 0122 loss_train: 0.3025 acc_train: 0.9292 loss_val: 0.4537 acc_val: 0.8838 time: 0.0434s\n",
      "Epoch: 0123 loss_train: 0.3022 acc_train: 0.9292 loss_val: 0.4538 acc_val: 0.8819 time: 0.0445s\n",
      "Epoch: 0124 loss_train: 0.3019 acc_train: 0.9292 loss_val: 0.4540 acc_val: 0.8838 time: 0.0499s\n",
      "Epoch: 0125 loss_train: 0.3017 acc_train: 0.9292 loss_val: 0.4542 acc_val: 0.8838 time: 0.0483s\n",
      "Epoch: 0126 loss_train: 0.3014 acc_train: 0.9298 loss_val: 0.4542 acc_val: 0.8838 time: 0.0439s\n",
      "Epoch: 0127 loss_train: 0.3011 acc_train: 0.9298 loss_val: 0.4544 acc_val: 0.8838 time: 0.0444s\n",
      "Epoch: 0128 loss_train: 0.3009 acc_train: 0.9298 loss_val: 0.4546 acc_val: 0.8838 time: 0.0501s\n",
      "Epoch: 0129 loss_train: 0.3006 acc_train: 0.9298 loss_val: 0.4547 acc_val: 0.8838 time: 0.0459s\n",
      "Epoch: 0130 loss_train: 0.3004 acc_train: 0.9298 loss_val: 0.4547 acc_val: 0.8838 time: 0.0444s\n",
      "Epoch: 0131 loss_train: 0.3001 acc_train: 0.9298 loss_val: 0.4549 acc_val: 0.8838 time: 0.0494s\n",
      "Epoch: 0132 loss_train: 0.2999 acc_train: 0.9298 loss_val: 0.4550 acc_val: 0.8838 time: 0.0505s\n",
      "Epoch: 0133 loss_train: 0.2996 acc_train: 0.9305 loss_val: 0.4552 acc_val: 0.8838 time: 0.0486s\n",
      "Epoch: 0134 loss_train: 0.2994 acc_train: 0.9305 loss_val: 0.4553 acc_val: 0.8838 time: 0.0468s\n",
      "Epoch: 0135 loss_train: 0.2991 acc_train: 0.9305 loss_val: 0.4555 acc_val: 0.8838 time: 0.0477s\n",
      "Epoch: 0136 loss_train: 0.2989 acc_train: 0.9298 loss_val: 0.4555 acc_val: 0.8838 time: 0.0501s\n",
      "Epoch: 0137 loss_train: 0.2987 acc_train: 0.9298 loss_val: 0.4554 acc_val: 0.8838 time: 0.0433s\n",
      "Epoch: 0138 loss_train: 0.2985 acc_train: 0.9298 loss_val: 0.4554 acc_val: 0.8838 time: 0.0460s\n",
      "Epoch: 0139 loss_train: 0.2983 acc_train: 0.9305 loss_val: 0.4556 acc_val: 0.8838 time: 0.0482s\n",
      "Epoch: 0140 loss_train: 0.2980 acc_train: 0.9311 loss_val: 0.4560 acc_val: 0.8838 time: 0.0482s\n",
      "Epoch: 0141 loss_train: 0.2978 acc_train: 0.9305 loss_val: 0.4563 acc_val: 0.8838 time: 0.0481s\n",
      "Epoch: 0142 loss_train: 0.2976 acc_train: 0.9311 loss_val: 0.4563 acc_val: 0.8838 time: 0.0471s\n",
      "Epoch: 0143 loss_train: 0.2974 acc_train: 0.9311 loss_val: 0.4560 acc_val: 0.8838 time: 0.0430s\n",
      "Epoch: 0144 loss_train: 0.2972 acc_train: 0.9311 loss_val: 0.4563 acc_val: 0.8838 time: 0.0488s\n",
      "Epoch: 0145 loss_train: 0.2970 acc_train: 0.9305 loss_val: 0.4564 acc_val: 0.8838 time: 0.0478s\n",
      "Epoch: 0146 loss_train: 0.2968 acc_train: 0.9311 loss_val: 0.4565 acc_val: 0.8838 time: 0.0470s\n",
      "Epoch: 0147 loss_train: 0.2966 acc_train: 0.9311 loss_val: 0.4567 acc_val: 0.8838 time: 0.0441s\n",
      "Epoch: 0148 loss_train: 0.2964 acc_train: 0.9311 loss_val: 0.4569 acc_val: 0.8838 time: 0.0487s\n",
      "Epoch: 0149 loss_train: 0.2962 acc_train: 0.9323 loss_val: 0.4569 acc_val: 0.8838 time: 0.0478s\n",
      "Epoch: 0150 loss_train: 0.2961 acc_train: 0.9305 loss_val: 0.4567 acc_val: 0.8838 time: 0.0471s\n",
      "Epoch: 0151 loss_train: 0.2959 acc_train: 0.9311 loss_val: 0.4568 acc_val: 0.8838 time: 0.0441s\n",
      "Epoch: 0152 loss_train: 0.2957 acc_train: 0.9317 loss_val: 0.4573 acc_val: 0.8838 time: 0.0486s\n",
      "Epoch: 0153 loss_train: 0.2956 acc_train: 0.9298 loss_val: 0.4573 acc_val: 0.8838 time: 0.0477s\n",
      "Epoch: 0154 loss_train: 0.2954 acc_train: 0.9298 loss_val: 0.4573 acc_val: 0.8838 time: 0.0471s\n",
      "Epoch: 0155 loss_train: 0.2953 acc_train: 0.9311 loss_val: 0.4575 acc_val: 0.8838 time: 0.0441s\n",
      "Epoch: 0156 loss_train: 0.2951 acc_train: 0.9305 loss_val: 0.4572 acc_val: 0.8838 time: 0.0489s\n",
      "Epoch: 0157 loss_train: 0.2950 acc_train: 0.9305 loss_val: 0.4574 acc_val: 0.8838 time: 0.0485s\n",
      "Epoch: 0158 loss_train: 0.2949 acc_train: 0.9311 loss_val: 0.4578 acc_val: 0.8838 time: 0.0478s\n",
      "Epoch: 0159 loss_train: 0.2948 acc_train: 0.9311 loss_val: 0.4579 acc_val: 0.8838 time: 0.0455s\n",
      "Epoch: 0160 loss_train: 0.2947 acc_train: 0.9311 loss_val: 0.4578 acc_val: 0.8838 time: 0.0475s\n",
      "Epoch: 0161 loss_train: 0.2946 acc_train: 0.9311 loss_val: 0.4580 acc_val: 0.8838 time: 0.0471s\n",
      "Epoch: 0162 loss_train: 0.2944 acc_train: 0.9311 loss_val: 0.4580 acc_val: 0.8838 time: 0.0475s\n",
      "Epoch: 0163 loss_train: 0.2943 acc_train: 0.9311 loss_val: 0.4579 acc_val: 0.8838 time: 0.0482s\n",
      "Epoch: 0164 loss_train: 0.2942 acc_train: 0.9311 loss_val: 0.4581 acc_val: 0.8838 time: 0.0491s\n",
      "Epoch: 0165 loss_train: 0.2940 acc_train: 0.9311 loss_val: 0.4584 acc_val: 0.8838 time: 0.0467s\n",
      "Epoch: 0166 loss_train: 0.2939 acc_train: 0.9298 loss_val: 0.4585 acc_val: 0.8838 time: 0.0492s\n",
      "Epoch: 0167 loss_train: 0.2938 acc_train: 0.9298 loss_val: 0.4587 acc_val: 0.8838 time: 0.0439s\n",
      "Epoch: 0168 loss_train: 0.2937 acc_train: 0.9305 loss_val: 0.4588 acc_val: 0.8838 time: 0.0479s\n",
      "Epoch: 0169 loss_train: 0.2937 acc_train: 0.9298 loss_val: 0.4588 acc_val: 0.8838 time: 0.0476s\n",
      "Epoch: 0170 loss_train: 0.2936 acc_train: 0.9305 loss_val: 0.4590 acc_val: 0.8838 time: 0.0457s\n",
      "Epoch: 0171 loss_train: 0.2934 acc_train: 0.9298 loss_val: 0.4592 acc_val: 0.8838 time: 0.0445s\n",
      "Epoch: 0172 loss_train: 0.2934 acc_train: 0.9298 loss_val: 0.4593 acc_val: 0.8838 time: 0.0482s\n",
      "Epoch: 0173 loss_train: 0.2932 acc_train: 0.9298 loss_val: 0.4594 acc_val: 0.8838 time: 0.0478s\n",
      "Epoch: 0174 loss_train: 0.2931 acc_train: 0.9311 loss_val: 0.4594 acc_val: 0.8838 time: 0.0474s\n",
      "Epoch: 0175 loss_train: 0.2930 acc_train: 0.9311 loss_val: 0.4593 acc_val: 0.8838 time: 0.0436s\n",
      "Epoch: 0176 loss_train: 0.2930 acc_train: 0.9311 loss_val: 0.4595 acc_val: 0.8838 time: 0.0483s\n",
      "Epoch: 0177 loss_train: 0.2929 acc_train: 0.9305 loss_val: 0.4599 acc_val: 0.8838 time: 0.0486s\n",
      "Epoch: 0178 loss_train: 0.2928 acc_train: 0.9298 loss_val: 0.4598 acc_val: 0.8838 time: 0.0467s\n",
      "Epoch: 0179 loss_train: 0.2927 acc_train: 0.9298 loss_val: 0.4599 acc_val: 0.8838 time: 0.0446s\n",
      "Epoch: 0180 loss_train: 0.2926 acc_train: 0.9305 loss_val: 0.4600 acc_val: 0.8838 time: 0.0483s\n",
      "Epoch: 0181 loss_train: 0.2925 acc_train: 0.9305 loss_val: 0.4601 acc_val: 0.8838 time: 0.0476s\n",
      "Epoch: 0182 loss_train: 0.2924 acc_train: 0.9311 loss_val: 0.4602 acc_val: 0.8838 time: 0.0440s\n",
      "Epoch: 0183 loss_train: 0.2924 acc_train: 0.9311 loss_val: 0.4602 acc_val: 0.8838 time: 0.0487s\n",
      "Epoch: 0184 loss_train: 0.2923 acc_train: 0.9311 loss_val: 0.4607 acc_val: 0.8838 time: 0.0484s\n",
      "Epoch: 0185 loss_train: 0.2923 acc_train: 0.9311 loss_val: 0.4606 acc_val: 0.8838 time: 0.0468s\n",
      "Epoch: 0186 loss_train: 0.2922 acc_train: 0.9305 loss_val: 0.4605 acc_val: 0.8838 time: 0.0454s\n",
      "Epoch: 0187 loss_train: 0.2921 acc_train: 0.9317 loss_val: 0.4606 acc_val: 0.8838 time: 0.0484s\n",
      "Epoch: 0188 loss_train: 0.2919 acc_train: 0.9305 loss_val: 0.4604 acc_val: 0.8838 time: 0.0477s\n",
      "Epoch: 0189 loss_train: 0.2919 acc_train: 0.9305 loss_val: 0.4604 acc_val: 0.8838 time: 0.0432s\n",
      "Epoch: 0190 loss_train: 0.2919 acc_train: 0.9305 loss_val: 0.4608 acc_val: 0.8838 time: 0.0464s\n",
      "Epoch: 0191 loss_train: 0.2918 acc_train: 0.9311 loss_val: 0.4609 acc_val: 0.8838 time: 0.0493s\n",
      "Epoch: 0192 loss_train: 0.2917 acc_train: 0.9311 loss_val: 0.4611 acc_val: 0.8838 time: 0.0484s\n",
      "Epoch: 0193 loss_train: 0.2916 acc_train: 0.9311 loss_val: 0.4610 acc_val: 0.8838 time: 0.0514s\n",
      "Epoch: 0194 loss_train: 0.2916 acc_train: 0.9305 loss_val: 0.4608 acc_val: 0.8819 time: 0.0492s\n",
      "Epoch: 0195 loss_train: 0.2914 acc_train: 0.9311 loss_val: 0.4612 acc_val: 0.8801 time: 0.0431s\n",
      "Epoch: 0196 loss_train: 0.2913 acc_train: 0.9311 loss_val: 0.4614 acc_val: 0.8801 time: 0.0447s\n",
      "Epoch: 0197 loss_train: 0.2913 acc_train: 0.9305 loss_val: 0.4612 acc_val: 0.8819 time: 0.0468s\n",
      "Epoch: 0198 loss_train: 0.2911 acc_train: 0.9311 loss_val: 0.4614 acc_val: 0.8801 time: 0.0472s\n",
      "Epoch: 0199 loss_train: 0.2911 acc_train: 0.9329 loss_val: 0.4616 acc_val: 0.8801 time: 0.0476s\n",
      "Epoch: 0200 loss_train: 0.2911 acc_train: 0.9317 loss_val: 0.4612 acc_val: 0.8782 time: 0.0480s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 12.4667s\n",
      "Loading 114th epoch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = 10000 + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(200):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == 100:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "#compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.4073 accuracy= 0.8928\n"
     ]
    }
   ],
   "source": [
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "def cal_ego_weighted_m(m,beta):\n",
    "    ego_weighted_list = []\n",
    "    for row in m:\n",
    "        new_row = (np.sum(adj[np.nonzero(row)[0]],axis=0)-1)*row\n",
    "        ego_weighted_list.append(new_row)\n",
    "    ego_weighted_arr = np.array(ego_weighted_list).astype(np.int).astype(np.float32)\n",
    "    ego_weighted_arr = np.power(ego_weighted_arr,beta)\n",
    "    ego_weighted_arr = sp.coo_matrix(ego_weighted_arr)\n",
    "    return ego_weighted_arr\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(\"/home/zhihao/Document/gnn_fd/graphSage/data/cora/raw/\", \"cora\"),\n",
    "                                        dtype=np.dtype(str))\n",
    "    #features_1 = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    #labels_1 = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(\"/home/zhihao/Document/gnn_fd/graphSage/data/cora/raw/\", \"cora\"),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    adj = adj.toarray()\n",
    "    adj = adj + np.eye(adj.shape[0],dtype=np.float32)\n",
    "    adj = cal_ego_weighted_m(adj,0.6)\n",
    "    \n",
    "    adj = normalize(adj)\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1823, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.3135,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.3648, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3648, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3919]])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch-gpu]",
   "language": "python",
   "name": "conda-env-.conda-torch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
